{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAXC16IcTRfp",
    "outputId": "5e91094d-92b1-4818-e8ab-1fd93a0cd8f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# COLAB에서만 동작하는 코드\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTgtpfC4eHuP"
   },
   "source": [
    "# 텍스트 분류\n",
    "\n",
    "copied and modified from https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WERUuDeQCLq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KVZ8aq96QC0Y"
   },
   "outputs": [],
   "source": [
    "## 설정\n",
    "VOCA_SIZE = 4000 # 어휘 사전의 크기\n",
    "EMBEDDING_SIZE = 64 # 단어를 임베딩한 벡터 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xeAkpUYJPpy"
   },
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nglK_9Hd3_r",
    "outputId": "d70df813-c6a1-4169-d581-bc1490db5d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\JuNoe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JuNoe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('Loading data...')\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.imdb.load_data(num_words=VOCA_SIZE)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B3TWo3_JR2U"
   },
   "source": [
    "## 데이터 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTUxzKwFJigM",
    "outputId": "0c85e366-0904-4620-8b46-941d15f4eb3a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 2, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 2, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 3715, 761, 61, 2, 452, 2, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 2, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 2, 4, 1615, 5, 2, 7, 2, 17, 13, 2, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 2, 192, 5, 1219, 3890, 19, 2, 217, 2, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 2, 5, 27, 2, 159, 90, 263, 2311, 2, 309, 8, 178, 5, 82, 2, 4, 65, 15, 2, 145, 143, 2, 12, 2, 537, 746, 537, 537, 15, 2, 4, 2, 594, 7, 2, 94, 2, 3987, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 2, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 2, 27, 2, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 2, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 3353, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2070, 307, 22, 7, 2, 126, 93, 40, 2, 13, 188, 1076, 3222, 19, 4, 2, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 2, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 2, 132, 8, 67, 6, 22, 15, 9, 283, 8, 2, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 2, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 2, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 2, 6, 226, 251, 7, 61, 113])]\n",
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[:5])\n",
    "print(train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ifl7K71wNchw"
   },
   "source": [
    "## 텍스트로 데이터 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkw9MdgJLZOX",
    "outputId": "d7b60d07-0c93-4ffc-ee18-7ed4c3006c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "<START> this film was just brilliant casting location scenery story direction <UNK> really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> island as myself so i loved the fact there was a real connection with this film the witty <UNK> throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was <UNK> life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "# 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "# word_index = {'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, ... }\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# reverse_word_index = {34704: 'fawn', 52009: 'tsukino', 52010: 'nunnery', ... }\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(train_x[0])\n",
    "print(decode_review(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Oxp3l0TJzgq"
   },
   "source": [
    "## 각 데이터의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sx0jFfDqHh4q",
    "outputId": "6a3f350c-28a3-4acf-e5f1-29f1cdb58515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n",
      "141\n",
      "550\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[0]))\n",
    "print(len(train_x[1]))\n",
    "print(len(train_x[2]))\n",
    "print(len(train_x[3]))\n",
    "print(len(train_x[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICmUFp5tNmGp"
   },
   "source": [
    "## 데이터 길이 일정하게 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpkgDH3CO4gG",
    "outputId": "466666bd-1cfc-4a62-ac4e-9bdcd220c8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(len(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C6fLRb_HA8A",
    "outputId": "b4bec9e1-9add-45df-ca1b-381719f65163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 400)\n",
      "(25000, 400)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=400, padding='post')\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=400, padding='post')\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6gb4O2OeWcO",
    "outputId": "6c427811-d062-45f3-973e-d18a8a370287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458    2   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172    2 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920    2  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117    2   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30    2   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16    2  113\n",
      "  103   32   15   16    2   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(len(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yoibr3xQN0Vq"
   },
   "source": [
    "## CNN 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYlvpK2neGdv",
    "outputId": "18cec54d-9382-4f30-ca11-2285aafa892f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 64)           256000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 398, 250)          48250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 367,251\n",
      "Trainable params: 367,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 13s 12ms/step - loss: 0.3999 - accuracy: 0.7985 - val_loss: 0.2688 - val_accuracy: 0.8872\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.2242 - accuracy: 0.9112 - val_loss: 0.2553 - val_accuracy: 0.8968\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.1589 - accuracy: 0.9398 - val_loss: 0.2643 - val_accuracy: 0.8944\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.1088 - accuracy: 0.9614 - val_loss: 0.3343 - val_accuracy: 0.8801\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0721 - accuracy: 0.9744 - val_loss: 0.3597 - val_accuracy: 0.8855\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0540 - accuracy: 0.9802 - val_loss: 0.4058 - val_accuracy: 0.8870\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0401 - accuracy: 0.9854 - val_loss: 0.4682 - val_accuracy: 0.8859\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0395 - accuracy: 0.9860 - val_loss: 0.4845 - val_accuracy: 0.8817\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0357 - accuracy: 0.9874 - val_loss: 0.4478 - val_accuracy: 0.8877\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0268 - accuracy: 0.9903 - val_loss: 0.4726 - val_accuracy: 0.8878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e3c1aff0a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(400))\n",
    "model.add(Embedding(VOCA_SIZE, EMBEDDING_SIZE)) # 텍스트는 임베딩 해서 사용한다.\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=10, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28CiZH01d5RR",
    "outputId": "7ac40096-684f-4130-920a-3f7a7948a713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4726 - accuracy: 0.8878\n",
      "loss = 0.4725516438484192\n",
      "acc = 0.8877999782562256\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_x, test_y)\n",
    "print(\"loss =\", loss)\n",
    "print(\"acc =\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQnEiGI_Y-2o"
   },
   "source": [
    "## RNN 모델 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKWcjknZZAiF",
    "outputId": "c1c04343-a35b-44f4-a73c-f9aaf365078b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 400, 64)           256000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               32250     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 354,549\n",
      "Trainable params: 354,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 33s 39ms/step - loss: 0.6259 - accuracy: 0.6416 - val_loss: 0.5568 - val_accuracy: 0.7496\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5811 - accuracy: 0.6962 - val_loss: 0.6490 - val_accuracy: 0.6440\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4381 - accuracy: 0.8059 - val_loss: 0.4438 - val_accuracy: 0.8006\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.3116 - accuracy: 0.8777 - val_loss: 0.3453 - val_accuracy: 0.8515\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.2596 - accuracy: 0.9031 - val_loss: 0.4948 - val_accuracy: 0.8293\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.2584 - accuracy: 0.9020 - val_loss: 0.3002 - val_accuracy: 0.8756\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.2017 - accuracy: 0.9264 - val_loss: 0.3308 - val_accuracy: 0.8698\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.1782 - accuracy: 0.9363 - val_loss: 0.3221 - val_accuracy: 0.8747\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.1554 - accuracy: 0.9463 - val_loss: 0.3435 - val_accuracy: 0.8802\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.1300 - accuracy: 0.9556 - val_loss: 0.3633 - val_accuracy: 0.8779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e3c73731f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(400))\n",
    "model.add(Embedding(VOCA_SIZE, EMBEDDING_SIZE)) # 자연어처리 위해 넣은 코드\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(250, 3))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Bidirectional(CuDNNLSTM(64))) # COLAB RNN 코드\n",
    "model.add(Bidirectional(LSTM(64))) # 윈도우 RNN 코드 CuDNNLSTM은 내장되있음\n",
    "model.add(Dense(250))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=10, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nd53lZYFZY0U",
    "outputId": "3e5ae600-a813-4320-849b-deded9a36085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 12ms/step - loss: 0.3633 - accuracy: 0.8779\n",
      "loss = 0.3632696866989136\n",
      "acc = 0.8779199719429016\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_x, test_y)\n",
    "print(\"loss =\", loss)\n",
    "print(\"acc =\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQyQSM1UVCh7"
   },
   "source": [
    "# Word2Vec 사용\n",
    "\n",
    "새로 Embedding을 학습하지 않고 이미 학습된 Word2Vec을 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrFo9E4cYDmR",
    "outputId": "de756c69-b69e-4ce5-e92e-39f46ed5b5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKDrAvegcSAN"
   },
   "source": [
    "아래 코드로 로딩이 가능한데, 데이터가 엄청 크다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T75NMTTjZYm9"
   },
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TUWPXMEcVZF"
   },
   "source": [
    "대신 슬림한 것을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNTK-z2tbsK9",
    "outputId": "4640cd07-d706-49a3-a204-17e604873dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-28 06:00:54--  https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz [following]\n",
      "--2020-11-28 06:00:54--  https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 276467217 (264M) [application/octet-stream]\n",
      "Saving to: ‘GoogleNews-vectors-negative300-SLIM.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>] 263.66M   197MB/s    in 1.3s    \n",
      "\n",
      "2020-11-28 06:01:01 (197 MB/s) - ‘GoogleNews-vectors-negative300-SLIM.bin.gz’ saved [276467217/276467217]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_PRDWT5b9fi"
   },
   "outputs": [],
   "source": [
    "!gzip -d GoogleNews-vectors-negative300-SLIM.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8JtCskE5bzYM"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300-SLIM.bin\", binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWIBrUKmciet",
    "outputId": "716ca9be-e477-48f9-c725-8a6a564dc1d6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.95743926e-02  5.22915944e-02 -5.08934222e-02  4.75378111e-02\n",
      "  4.19451296e-02  7.27048889e-03  1.43312523e-02 -1.00668306e-02\n",
      "  7.27048889e-02  6.37915509e-04 -9.33934498e-05 -5.87231815e-02\n",
      " -8.55680630e-02 -1.06260993e-01 -1.27513185e-01 -5.95620833e-02\n",
      " -5.42490333e-02 -1.63848163e-04 -7.01881796e-02 -5.98417185e-02\n",
      "  7.21456185e-02 -1.06820263e-01  5.48083000e-02 -5.94222639e-03\n",
      " -8.50087926e-02  3.55135426e-02 -1.31987333e-01  4.08265926e-02\n",
      "  2.04692230e-01  3.39755528e-02 -4.22247648e-02 -3.29968333e-02\n",
      " -2.58661620e-02 -2.43281741e-02 -9.00422111e-02  4.41822037e-02\n",
      " -5.64861074e-02  8.27717185e-02  9.33978185e-02  7.99753815e-02\n",
      " -6.11699792e-03 -2.22309176e-02  3.71913463e-02  5.14526926e-02\n",
      "  8.16531852e-02 -7.01881796e-02 -2.06929296e-02 -3.10393963e-02\n",
      "  1.13531485e-01  7.88568407e-02 -8.44495296e-02  8.22124556e-02\n",
      " -2.67400197e-03 -6.15195222e-02  2.96412241e-02 -3.80302519e-02\n",
      "  5.45286685e-02 -7.27048889e-02  1.09616600e-01  1.02765569e-02\n",
      "  2.88023222e-02  2.99208593e-02  4.72581796e-02 -6.29176944e-02\n",
      "  3.94284204e-02 -5.39693981e-02  2.01336611e-02 -2.57263463e-02\n",
      " -6.20787926e-02 -2.15842645e-03 -2.48874426e-02  1.70576852e-02\n",
      "  2.01336611e-02 -1.68479607e-02 -1.33525329e-02 -1.04163736e-02\n",
      "  3.29968333e-02 -9.73126963e-02 -2.49923067e-03 -5.52714453e-04\n",
      " -5.38295833e-03 -5.45286685e-02  4.69785444e-02  7.27048889e-02\n",
      " -1.00109041e-01 -2.97810417e-02 -2.09725648e-02 -6.01213500e-02\n",
      " -2.25105528e-02 -3.36959213e-02 -1.18844528e-02  1.24157585e-01\n",
      " -1.87354907e-02  1.68479607e-02 -8.55680630e-02 -1.21920511e-01\n",
      "  5.28508611e-02  3.45348231e-02  5.17323241e-02 -7.27048889e-02\n",
      " -2.19512843e-02 -4.41822037e-02 -7.37535162e-03 -4.75378111e-02\n",
      "  6.51547685e-02 -8.44495296e-02 -2.76837852e-02 -3.48144583e-02\n",
      "  3.28570195e-02 -3.88691537e-02 -1.48765385e-01  2.40485407e-02\n",
      "  8.45893472e-03  8.27717185e-02 -5.62064722e-02 -2.47476269e-02\n",
      " -4.13858593e-02 -1.39257833e-01 -7.88568407e-02 -8.38902593e-03\n",
      " -5.55772940e-03  5.56472056e-02 -1.04583189e-01 -3.65272164e-03\n",
      " -4.11062278e-02  1.78965889e-02  1.09616600e-01 -2.41883583e-02\n",
      "  4.30636667e-02 -1.06260993e-01 -4.08265926e-02  3.48144583e-02\n",
      " -9.28385556e-02  4.27840315e-02 -6.48751333e-02  5.36897667e-02\n",
      "  2.62856148e-02  6.81608357e-03  2.60059796e-02  1.69877764e-02\n",
      "  5.36897667e-02 -8.83644074e-02 -2.12521981e-02  1.90151259e-02\n",
      "  1.37719838e-02  8.72458667e-02 -5.00545204e-02  8.10939148e-02\n",
      "  4.31685289e-03 -2.99208593e-02  1.33105874e-01  4.39025685e-02\n",
      " -7.88568407e-02  5.84435463e-02  5.48083000e-02 -7.15863556e-02\n",
      "  3.46047315e-03 -1.01367394e-02  4.19451296e-02 -5.17323241e-02\n",
      "  3.57931778e-02 -5.90028130e-02  6.06806204e-02 -1.69877764e-02\n",
      "  2.48175347e-03 -2.33494546e-02  7.71790370e-02 -7.21456185e-02\n",
      " -7.79480301e-03  3.57931778e-02 -2.65652481e-02 -2.05531139e-02\n",
      "  1.27233556e-02  3.71913463e-02  2.01336611e-02 -2.39087231e-02\n",
      "  1.06260991e-02  2.71245167e-02 -5.73250093e-02 -9.33978185e-02\n",
      " -7.88568407e-02 -6.82307407e-02 -3.50940898e-02 -3.77506167e-02\n",
      "  1.62886921e-02 -6.68325722e-02  2.98859039e-03  5.34101315e-02\n",
      "  6.68325722e-02  1.56595148e-02 -1.92947593e-02 -1.24716848e-01\n",
      " -7.43826926e-02  5.14526926e-02  3.08995787e-02  1.18844528e-02\n",
      "  5.59268380e-03 -7.49419630e-02 -4.19451296e-02  1.71275940e-02\n",
      "  3.66320796e-02 -4.19451296e-02  1.28456962e-03 -4.25043963e-02\n",
      "  3.27172019e-02  6.68500492e-04  1.01227574e-01 -6.51547685e-02\n",
      "  3.22977491e-02 -1.17446361e-02 -1.52121007e-01 -2.08327472e-02\n",
      "  3.94284204e-02 -1.76728815e-01 -3.48144583e-02  8.44495296e-02\n",
      "  1.09057337e-01 -1.11154588e-02  1.71975028e-02  6.73918426e-02\n",
      "  3.80302519e-02  8.17930046e-03 -7.55012333e-02 -3.97080556e-02\n",
      "  4.61396426e-02 -3.07597611e-02 -3.02004926e-02  3.36959213e-02\n",
      "  8.83644074e-02 -1.04862824e-02  2.46078093e-02 -4.27840315e-02\n",
      "  4.66989093e-02  5.43975912e-04 -3.74709815e-02 -3.85895185e-02\n",
      " -1.64285097e-02  2.11123824e-02 -3.17384824e-02 -3.52339074e-02\n",
      "  7.71790370e-02 -8.10939148e-02 -2.41883583e-02 -6.93492815e-02\n",
      "  4.39025685e-02  2.83129630e-03  9.15801991e-03 -5.95620833e-02\n",
      "  8.61273333e-02 -5.31304954e-03 -7.77383074e-02  5.53675704e-02\n",
      " -2.82430537e-02  3.57931778e-02 -4.71882708e-03  1.78965889e-02\n",
      "  2.42932211e-03 -5.17323241e-02 -6.12398870e-02 -4.75378111e-02\n",
      " -7.94161111e-02  3.97080556e-02  1.61069289e-01  1.07659167e-02\n",
      "  6.29176944e-02  6.12398870e-02 -1.39257833e-01 -2.82430537e-02\n",
      " -8.94829445e-03 -5.67657426e-02  8.69487587e-04  1.27233556e-02\n",
      "  1.15349106e-02  1.00668311e-01  7.15863556e-02  2.86625046e-02\n",
      "  5.54899103e-04  1.53798806e-02 -5.84435463e-02  1.45934091e-03\n",
      "  7.55012333e-02 -2.54467111e-02 -5.87231815e-02 -3.07597611e-02\n",
      "  3.66320796e-02 -2.60059796e-02 -3.04801278e-02  3.86244734e-03\n",
      "  8.94829407e-02 -6.64131204e-03 -3.02004926e-02 -1.44710699e-02\n",
      "  4.19451296e-02 -7.51516875e-03  2.58661620e-02  1.13251852e-02\n",
      "  8.42398033e-03  5.45286685e-02 -2.92217731e-02  4.94952537e-02]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec['tiger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec['tiger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_0GYR8jcb1g",
    "outputId": "14711b5c-67e3-4af3-b1da-8a27d197d0ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tigers', 0.8028031587600708),\n",
       " ('elephant', 0.6681443452835083),\n",
       " ('rhino', 0.6406095027923584),\n",
       " ('elephants', 0.6400991678237915),\n",
       " ('panther', 0.6312947273254395),\n",
       " ('leopard', 0.6132040619850159),\n",
       " ('tigress', 0.5982028245925903),\n",
       " ('cheetah', 0.5816307663917542),\n",
       " ('lions', 0.5742772817611694),\n",
       " ('gorilla', 0.5742713212966919)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('tiger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WovKk-gvWCKK",
    "outputId": "4c3e809e-02f0-4432-a26c-bef44f193e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "  words = decode_review(train_x[i]).split(\" \")\n",
    "  word_set.update(words)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "  words = decode_review(test_x[i]).split(\" \")\n",
    "  word_set.update(words)\n",
    "\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGBtwK9Mc6fj",
    "outputId": "597e1c54-63a5-402c-ee96-bdda6bb0ba4c"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = len(word2vec['tiger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnqbiMxuVJEu",
    "outputId": "6e2c7b5e-cde2-4abd-df0f-9e0f522cbae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "VOCA_SIZE = 4000 # 어휘 사전의 크기\n",
    "EMBEDDING_SIZE = len(word2vec['tiger'])\n",
    "# EMBEDDING_SIZE = 300 # 단어를 임베딩한 벡터 크기\n",
    "\n",
    "embedding_matrix = np.zeros((VOCA_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# tokenizer에 있는 단어 사전을 순회하면서 word2vec의 300차원 vector를 가져옵니다\n",
    "for idx, word in enumerate(word_set):\n",
    "    embedding_vector = word2vec[word] if word in word2vec else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ValNpKi1Xwzw",
    "outputId": "bb799820-1fa3-4f79-9183-7fd869032186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 300)          1200000   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 400, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 250)               32250     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,419,381\n",
      "Trainable params: 219,381\n",
      "Non-trainable params: 1,200,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 45s 55ms/step - loss: 0.6701 - accuracy: 0.5843 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.6670 - accuracy: 0.5837 - val_loss: 0.6617 - val_accuracy: 0.5526\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.6499 - accuracy: 0.6216 - val_loss: 0.5999 - val_accuracy: 0.6766\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.6173 - accuracy: 0.6629 - val_loss: 0.6375 - val_accuracy: 0.6354\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.5880 - accuracy: 0.6922 - val_loss: 0.5354 - val_accuracy: 0.7306\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.5982 - accuracy: 0.6673 - val_loss: 0.6280 - val_accuracy: 0.6420\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.5557 - accuracy: 0.7068 - val_loss: 0.5783 - val_accuracy: 0.7160\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.4924 - accuracy: 0.7580 - val_loss: 0.4618 - val_accuracy: 0.7769\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 41s 53ms/step - loss: 0.4636 - accuracy: 0.7754 - val_loss: 0.4503 - val_accuracy: 0.7842\n",
      "Epoch 10/10\n",
      " 39/782 [>.............................] - ETA: 26s - loss: 0.4551 - accuracy: 0.7869"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(400))\n",
    "# model.add(Embedding(VOCA_SIZE, EMBEDDING_SIZE))\n",
    "model.add(Embedding(VOCA_SIZE, \n",
    "                    EMBEDDING_SIZE, \n",
    "                    input_length=400, \n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False\n",
    "                   )\n",
    "         )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(250))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=10, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bwf4ZgPmdObl",
    "outputId": "7a7e8cc6-6781-4c9c-9c07-2456d08fb7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 17ms/step - loss: 0.5450 - accuracy: 0.7220\n",
      "loss = 0.5449949502944946\n",
      "acc = 0.7220399975776672\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_x, test_y)\n",
    "print(\"loss =\", loss)\n",
    "print(\"acc =\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzIhPwludp-9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "rnn_text_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
